#!/usr/bin/env python3
"""
Code Churn Tracker - è¿½è¸ªä»£ç å˜æ›´ç‡ï¼Œè¯†åˆ«é«˜é¢‘ä¿®æ”¹æ–‡ä»¶
"""

import subprocess
import re
import os
from datetime import datetime, timedelta
from collections import defaultdict
from pathlib import Path

def parse_git_date(date_str):
    """è§£æ Git æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¤„ç†å„ç§æ ¼å¼"""
    # Git format: "2026-01-30 19:58:09 +0800"
    # Python < 3.11 çš„ fromisoformat ä¸æ”¯æŒå¸¦ç©ºæ ¼çš„ ISO æ ¼å¼
    # ä½¿ç”¨ strptime ä½œä¸ºå¯é çš„æ›¿ä»£æ–¹æ¡ˆ
    try:
        # å°è¯•æ ‡å‡† ISO æ ¼å¼ï¼ˆPython 3.11+ï¼‰
        return datetime.fromisoformat(date_str)
    except ValueError:
        # å›é€€åˆ° strptimeï¼ˆé€‚ç”¨äºæ‰€æœ‰ Python ç‰ˆæœ¬ï¼‰
        return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S %z')

# éœ€è¦æ’é™¤çš„æ–‡ä»¶æ¨¡å¼
EXCLUDE_PATTERNS = [
    r'node_modules/',
    r'vendor/',
    r'\.git/',
    r'dist/',
    r'build/',
    r'\.venv/',
    r'venv/',
    r'__pycache__/',
    r'\.pyc$',
    r'\.min\.js$',
    r'\.min\.css$',
    r'package-lock\.json',
    r'yarn\.lock',
    r'Pods/',
    r'\.xcodeproj/',
    r'\.xcworkspace/',
    r'DerivedData/',
]

def should_exclude(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
    for pattern in EXCLUDE_PATTERNS:
        if re.search(pattern, file_path):
            return True
    return False

def get_git_root():
    """è·å– Git ä»“åº“æ ¹ç›®å½•"""
    result = subprocess.run(
        ['git', 'rev-parse', '--show-toplevel'],
        capture_output=True,
        text=True
    )
    if result.returncode != 0:
        return None
    return result.stdout.strip()

def get_file_size(file_path, git_root):
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
    full_path = os.path.join(git_root, file_path)
    if os.path.exists(full_path):
        return os.path.getsize(full_path)
    return 0

def get_git_commits(days=90):
    """è·å–æŒ‡å®šå¤©æ•°å†…çš„ Git æäº¤å†å²"""
    since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

    result = subprocess.run([
        'git', 'log',
        f'--since={since_date}',
        '--name-status',
        '--pretty=format:%H|%ai|%an',
        '-m',
    ], capture_output=True, text=True)

    if result.returncode != 0:
        return []

    lines = result.stdout.strip().split('\n')
    commits = []
    current_commit = None

    for line in lines:
        if not line:
            continue

        if '|' in line:
            # æäº¤ä¿¡æ¯è¡Œ
            parts = line.split('|')
            if len(parts) == 3:
                current_commit = {
                    'hash': parts[0],
                    'date': parts[1],
                    'author': parts[2],
                    'files': []
                }
                commits.append(current_commit)
        elif current_commit and line:
            # æ–‡ä»¶å˜æ›´è¡Œ
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                status = parts[0]
                file_path = parts[1]
                if not should_exclude(file_path):
                    current_commit['files'].append((status, file_path))

    return commits

def analyze_commits(commits, git_root):
    """åˆ†ææäº¤å†å²ï¼Œè®¡ç®—ä»£ç å˜æ›´æŒ‡æ ‡"""
    file_stats = defaultdict(lambda: {
        'commits': 0,
        'additions': 0,
        'deletions': 0,
        'modifications': 0,
        'renames': 0,
        'first_commit': None,
        'last_commit': None,
        'size': 0,
        'authors': set(),
    })

    if not commits:
        return {}, {}

    total_commits = len(commits)
    start_date = parse_git_date(commits[-1]['date'])
    end_date = parse_git_date(commits[0]['date'])
    days_span = max(1, (end_date - start_date).days)

    for commit in commits:
        commit_date = parse_git_date(commit['date'])
        author = commit['author']

        for status, file_path in commit['files']:
            if should_exclude(file_path):
                continue

            stats = file_stats[file_path]

            if stats['first_commit'] is None:
                stats['first_commit'] = commit_date
            stats['last_commit'] = commit_date
            stats['authors'].add(author)

            if status == 'A':
                stats['additions'] += 1
            elif status == 'D':
                stats['deletions'] += 1
            elif status == 'M':
                stats['modifications'] += 1
            elif status.startswith('R'):
                stats['renames'] += 1

            stats['commits'] += 1
            stats['size'] = get_file_size(file_path, git_root)

    # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
    stability_scores = {}
    for file_path, stats in file_stats.items():
        churn_rate = stats['commits'] / days_span if days_span > 0 else 0
        max_commits = total_commits

        # ç¨³å®šæ€§è¯„åˆ†ï¼š100 = éå¸¸ç¨³å®šï¼Œ0 = éå¸¸ä¸ç¨³å®š
        # è€ƒè™‘å› ç´ ï¼šæäº¤æ¬¡æ•°ã€ä¿®æ”¹é¢‘ç‡ã€æ–‡ä»¶å¤§å°
        base_score = 100 - (stats['commits'] / max_commits * 50)
        churn_penalty = min(churn_rate * 100, 30)
        size_factor = min(stats['size'] / 100000 * 10, 20)  # å¤§æ–‡ä»¶æ›´å®¹æ˜“ä¸ç¨³å®š

        stability = max(0, min(100, base_score - churn_penalty - size_factor))
        stability_scores[file_path] = int(stability)

    return dict(file_stats), stability_scores

def generate_report(file_stats, stability_scores, commits, days=90):
    """ç”Ÿæˆä»£ç å˜æ›´ç‡åˆ†ææŠ¥å‘Š"""
    now = datetime.now()
    since_date = (now - timedelta(days=days)).strftime('%Y-%m-%d')

    report = []
    report.append("=" * 140)
    report.append("ä»£ç å˜æ›´ç‡åˆ†ææŠ¥å‘Š (Code Churn Analysis)")
    report.append(f"åˆ†ææ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"åˆ†æå‘¨æœŸ: {since_date} è‡³ä»Š ({days} å¤©)")
    report.append("=" * 140)

    if not commits:
        report.append("")
        report.append("âš ï¸  åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æäº¤è®°å½•")
        report.append("")
        report.append("å¯èƒ½åŸå› :")
        report.append("  - ä»“åº“æ˜¯æ–°å»ºçš„ï¼Œè¿˜æ²¡æœ‰æäº¤")
        report.append("  - æŒ‡å®šçš„æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ´»åŠ¨")
        report.append("  - ä¸æ˜¯ Git ä»“åº“")
        return '\n'.join(report)

    total_commits = len(commits)
    total_files = len(file_stats)

    report.append(f"æ€»æäº¤æ•°: {total_commits}")
    report.append(f"æ¶‰åŠæ–‡ä»¶æ•°: {total_files}")
    report.append("")

    # ç»Ÿè®¡æ‘˜è¦
    report.append("=" * 140)
    report.append("ğŸ“Š å˜æ›´ç»Ÿè®¡æ‘˜è¦")
    report.append("=" * 140)

    total_additions = sum(s['additions'] for s in file_stats.values())
    total_modifications = sum(s['modifications'] for s in file_stats.values())
    total_deletions = sum(s['deletions'] for s in file_stats.values())

    report.append(f"æ–°å¢æ–‡ä»¶: {total_additions}")
    report.append(f"ä¿®æ”¹æ“ä½œ: {total_modifications}")
    report.append(f"åˆ é™¤æ–‡ä»¶: {total_deletions}")
    report.append("")

    # ç¨³å®šæ€§åˆ†å¸ƒ
    high_stability = sum(1 for s in stability_scores.values() if s >= 80)
    medium_stability = sum(1 for s in stability_scores.values() if 50 <= s < 80)
    low_stability = sum(1 for s in stability_scores.values() if s < 50)

    report.append("ç¨³å®šæ€§åˆ†å¸ƒ:")
    report.append(f"  ğŸŸ¢ é«˜ç¨³å®šæ€§ (80-100): {high_stability} ä¸ªæ–‡ä»¶")
    report.append(f"  ğŸŸ¡ ä¸­ç¨³å®šæ€§ (50-79):  {medium_stability} ä¸ªæ–‡ä»¶")
    report.append(f"  ğŸ”´ ä½ç¨³å®šæ€§ (0-49):   {low_stability} ä¸ªæ–‡ä»¶")
    report.append("")

    # é«˜å˜åŠ¨æ–‡ä»¶
    report.append("=" * 140)
    report.append("ğŸ”¥ é«˜å˜åŠ¨æ–‡ä»¶ (Top 20)")
    report.append("=" * 140)
    report.append(f"{'æ–‡ä»¶è·¯å¾„':<50} {'æäº¤æ•°':<8} {'ç¨³å®šæ€§':<10} {'å¤§å°':<12} {'ä¿®æ”¹è€…æ•°'}")
    report.append("-" * 140)

    sorted_by_churn = sorted(
        file_stats.items(),
        key=lambda x: x[1]['commits'],
        reverse=True
    )

    for file_path, stats in sorted_by_churn[:20]:
        commits_count = stats['commits']
        stability = stability_scores[file_path]
        size = stats['size']
        authors = len(stats['authors'])

        # ç¨³å®šæ€§æŒ‡ç¤ºå™¨
        if stability >= 80:
            stability_indicator = f"ğŸŸ¢ {stability}"
        elif stability >= 50:
            stability_indicator = f"ğŸŸ¡ {stability}"
        else:
            stability_indicator = f"ğŸ”´ {stability}"

        # æ–‡ä»¶å¤§å°æ ¼å¼åŒ–
        if size >= 1024 * 1024:
            size_str = f"{size / (1024 * 1024):.1f} MB"
        elif size >= 1024:
            size_str = f"{size / 1024:.1f} KB"
        else:
            size_str = f"{size} B"

        # æˆªæ–­è¿‡é•¿çš„è·¯å¾„
        display_path = file_path if len(file_path) <= 48 else '...' + file_path[-45:]

        report.append(f"{display_path:<50} {commits_count:<8} {stability_indicator:<10} {size_str:<12} {authors}")

    report.append("")

    # é£é™©åŒºåŸŸåˆ†æ
    report.append("=" * 140)
    report.append("âš ï¸  é£é™©åŒºåŸŸè¯†åˆ«")
    report.append("=" * 140)

    high_risk = []
    for file_path, stats in file_stats.items():
        stability = stability_scores[file_path]
        churn_rate = stats['commits'] / days if days > 0 else 0
        size = stats['size']

        # é«˜é£é™©æ ‡å‡†ï¼šä½ç¨³å®šæ€§ + é«˜å˜åŠ¨ç‡ æˆ– å¤§æ–‡ä»¶ + é«˜å˜åŠ¨
        if (stability < 50 and churn_rate > 0.1) or (size > 50000 and stats['commits'] > 10):
            high_risk.append((file_path, stats, stability, churn_rate))

    high_risk.sort(key=lambda x: x[3], reverse=True)

    if high_risk:
        report.append(f"å‘ç° {len(high_risk)} ä¸ªé«˜é£é™©æ–‡ä»¶:")
        report.append("")
        for file_path, stats, stability, churn_rate in high_risk[:15]:
            risk_reason = []
            if stability < 50:
                risk_reason.append(f"ä½ç¨³å®šæ€§({stability})")
            if churn_rate > 0.1:
                risk_reason.append(f"é«˜é¢‘å˜åŠ¨({churn_rate:.2f}/å¤©)")
            if stats['size'] > 50000:
                risk_reason.append(f"å¤§æ–‡ä»¶({stats['size']//1024}KB)")

            report.append(f"  - {file_path}")
            report.append(f"    åŸå› : {', '.join(risk_reason)}")
            report.append(f"    æäº¤: {stats['commits']} æ¬¡ | ä¿®æ”¹è€…: {len(stats['authors'])} äºº")
            report.append("")
    else:
        report.append("âœ… æœªå‘ç°æ˜æ˜¾çš„é«˜é£é™©æ–‡ä»¶")
        report.append("")

    # æŒ‰æ–‡ä»¶ç±»å‹åˆ†æ
    report.append("=" * 140)
    report.append("ğŸ“ æŒ‰æ–‡ä»¶ç±»å‹åˆ†æ")
    report.append("=" * 140)

    ext_stats = defaultdict(lambda: {'files': 0, 'commits': 0, 'total_size': 0})
    for file_path, stats in file_stats.items():
        ext = Path(file_path).suffix or '(no extension)'
        ext_stats[ext]['files'] += 1
        ext_stats[ext]['commits'] += stats['commits']
        ext_stats[ext]['total_size'] += stats['size']

    sorted_exts = sorted(ext_stats.items(), key=lambda x: x[1]['commits'], reverse=True)
    for ext, data in sorted_exts[:10]:
        avg_size = data['total_size'] / data['files'] if data['files'] > 0 else 0
        report.append(f"  {ext:<20} æ–‡ä»¶: {data['files']:<4} | æäº¤: {data['commits']:<5} | å¹³å‡å¤§å°: {avg_size/1024:.1f} KB")

    report.append("")

    # å»ºè®®
    report.append("=" * 140)
    report.append("ğŸ’¡ æ”¹è¿›å»ºè®®")
    report.append("=" * 140)

    if low_stability > 0:
        report.append("")
        report.append("é’ˆå¯¹ä½ç¨³å®šæ€§æ–‡ä»¶:")
        report.append("  1. å®¡æŸ¥é¢‘ç¹ä¿®æ”¹çš„åŸå› ")
        report.append("  2. è€ƒè™‘é‡æ„è®¾è®¡ä»¥å‡å°‘å˜æ›´éœ€æ±‚")
        report.append("  3. å¢åŠ å•å…ƒæµ‹è¯•ä»¥æé«˜å˜æ›´ä¿¡å¿ƒ")
        report.append("  4. è¯„ä¼°æ˜¯å¦éœ€è¦æ‹†åˆ†å¤æ‚æ¨¡å—")

    if high_risk:
        report.append("")
        report.append("é’ˆå¯¹é«˜é£é™©æ–‡ä»¶:")
        report.append("  1. ä¼˜å…ˆè¿›è¡Œä»£ç å®¡æŸ¥")
        report.append("  2. è€ƒè™‘æ¨¡å—åŒ–æ‹†åˆ†")
        report.append("  3. å»ºç«‹æ›´ä¸¥æ ¼çš„æµ‹è¯•è¦†ç›–")
        report.append("  4. è¯„ä¼°æŠ€æœ¯å€ºåŠ¡å¿è¿˜è®¡åˆ’")

    report.append("")
    report.append("é€šç”¨å»ºè®®:")
    report.append("  - å®šæœŸè¿è¡Œæ­¤åˆ†æè·Ÿè¸ªä»£ç å¥åº·åº¦")
    report.append("  - åœ¨ä»£ç å®¡æŸ¥æ—¶å…³æ³¨é«˜å˜åŠ¨æ–‡ä»¶")
    report.append("  - å¯¹é¢‘ç¹å˜æ›´çš„åŒºåŸŸè¿›è¡Œæ¶æ„å®¡æŸ¥")
    report.append("  - è€ƒè™‘ä½¿ç”¨ç‰¹æ€§å¼€å…³å‡å°‘ç›´æ¥ä¿®æ”¹")

    return '\n'.join(report)

def save_report(report, output_file):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

def main():
    print("ğŸ” æ­£åœ¨åˆ†æ Git æäº¤å†å²...")

    git_root = get_git_root()
    if not git_root:
        print("âŒ é”™è¯¯: å½“å‰ç›®å½•ä¸æ˜¯ Git ä»“åº“")
        return

    print(f"âœ… Git ä»“åº“æ ¹ç›®å½•: {git_root}")

    commits = get_git_commits(days=90)
    print(f"âœ… è·å–åˆ° {len(commits)} ä¸ªæäº¤è®°å½•")

    if not commits:
        print("âš ï¸  åœ¨è¿‡å» 90 å¤©å†…æ²¡æœ‰æ‰¾åˆ°æäº¤è®°å½•")
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆç©ºæŠ¥å‘Š...")
        report = generate_report({}, {}, commits, days=90)
    else:
        print("ğŸ“Š æ­£åœ¨åˆ†ææ–‡ä»¶å˜æ›´...")
        file_stats, stability_scores = analyze_commits(commits, git_root)
        print(f"âœ… åˆ†æäº† {len(file_stats)} ä¸ªæ–‡ä»¶")

        print("ğŸ“ æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report = generate_report(file_stats, stability_scores, commits, days=90)

    output_file = 'code_churn_report.txt'
    save_report(report, output_file)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    print("\n" + "=" * 60)
    print("ğŸ“‹ åˆ†ææ‘˜è¦")
    print("=" * 60)
    print(f"  åˆ†æå‘¨æœŸ: æœ€è¿‘ 90 å¤©")
    print(f"  æ€»æäº¤æ•°: {len(commits)}")
    print(f"  æ¶‰åŠæ–‡ä»¶: {len(commits) > 0 and len(set(f for c in commits for _, f in c['files'])) or 0}")
    print(f"  æŠ¥å‘Šæ–‡ä»¶: {output_file}")

if __name__ == '__main__':
    main()
