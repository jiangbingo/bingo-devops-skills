#!/usr/bin/env python3
"""
ä»£ç å¤æ‚åº¦åˆ†æå·¥å…·
ä½¿ç”¨ radon (Python) æˆ– lizard (å¤šè¯­è¨€) åˆ†æä»£ç å¤æ‚åº¦
"""

import subprocess
import sys
import os
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict


def check_radon_installed():
    """æ£€æŸ¥ radon æ˜¯å¦å·²å®‰è£…"""
    try:
        result = subprocess.run(
            ['radon', '--version'],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except FileNotFoundError:
        return False


def check_lizard_installed():
    """æ£€æŸ¥ lizard æ˜¯å¦å·²å®‰è£…"""
    try:
        result = subprocess.run(
            ['lizard', '--version'],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except FileNotFoundError:
        return False


def analyze_with_radon():
    """ä½¿ç”¨ radon åˆ†æ Python ä»£ç å¤æ‚åº¦"""
    if not check_radon_installed():
        print("âš ï¸  radon æœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
        subprocess.run(['pip', 'install', 'radon'], capture_output=True)
        if not check_radon_installed():
            return None

    print("ğŸ” ä½¿ç”¨ radon åˆ†æ Python ä»£ç ...")

    try:
        # ä½¿ç”¨ radon åˆ†æåœˆå¤æ‚åº¦
        result = subprocess.run(
            ['radon', 'cc', '.', '-a', '-s', '-j'],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0:
            return parse_radon_output(result.stdout)
        else:
            # å°è¯•ä¸ä½¿ç”¨ JSON æ ¼å¼
            result = subprocess.run(
                ['radon', 'cc', '.', '-a', '-s'],
                capture_output=True,
                text=True,
                timeout=60
            )
            return parse_radon_text_output(result.stdout)

    except subprocess.TimeoutExpired:
        print("âš ï¸  radon åˆ†æè¶…æ—¶")
        return None
    except Exception as e:
        print(f"âš ï¸  radon åˆ†æå¤±è´¥: {e}")
        return None


def parse_radon_output(json_output):
    """è§£æ radon JSON è¾“å‡º"""
    import json

    try:
        data = json.loads(json_output)
        files = {}

        for file_path, file_data in data.items():
            if isinstance(file_data, dict) and 'classes' in file_data:
                for class_name, class_data in file_data['classes'].items():
                    for method_name, method_data in class_data['methods'].items():
                        full_name = f"{file_path}:{class_name}.{method_name}"
                        files[full_name] = {
                            'type': 'method',
                            'complexity': method_data['complexity'],
                            'lineno': method_data['lineno'],
                            'endline': method_data['endline'],
                            'file': file_path,
                            'class': class_name,
                            'method': method_name,
                        }

        return organize_complexity_data(files, 'radon')

    except json.JSONDecodeError:
        return None


def parse_radon_text_output(text_output):
    """è§£æ radon æ–‡æœ¬è¾“å‡º"""
    files = {}

    # radon è¾“å‡ºæ ¼å¼: FILE:lineno:lineno: CLASS.METHOD complexity -> CC
    pattern = r'^(.+?:(\d+):(\d+)):\s+(\S+)\s+(\S+)\s+->\s+(\d+)'

    for line in text_output.split('\n'):
        match = re.match(pattern, line)
        if match:
            location, lineno, endline, class_method, method_type, complexity = match.groups()

            # è§£æ class.method
            if '.' in class_method:
                parts = class_method.split('.')
                if len(parts) >= 2:
                    class_name = parts[0]
                    method_name = '.'.join(parts[1:])
                else:
                    class_name = ''
                    method_name = class_method
            else:
                class_name = ''
                method_name = class_method

            files[location] = {
                'type': 'method',
                'complexity': int(complexity),
                'lineno': int(lineno),
                'endline': int(endline),
                'file': location.split(':')[0] if ':' in location else '',
                'class': class_name,
                'method': method_name,
            }

    return organize_complexity_data(files, 'radon')


def analyze_with_lizard():
    """ä½¿ç”¨ lizard åˆ†æå¤šè¯­è¨€ä»£ç å¤æ‚åº¦"""
    if not check_lizard_installed():
        print("âš ï¸  lizard æœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
        subprocess.run(['pip', 'install', 'lizard'], capture_output=True)
        if not check_lizard_installed():
            return None

    print("ğŸ” ä½¿ç”¨ lizard åˆ†æä»£ç å¤æ‚åº¦...")

    try:
        result = subprocess.run(
            ['lizard', '.', '--CCN', '15'],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0:
            return parse_lizard_output(result.stdout)
        else:
            return None

    except subprocess.TimeoutExpired:
        print("âš ï¸  lizard åˆ†æè¶…æ—¶")
        return None
    except Exception as e:
        print(f"âš ï¸  lizard åˆ†æå¤±è´¥: {e}")
        return None


def parse_lizard_output(text_output):
    """è§£æ lizard è¾“å‡º"""
    files = {}
    functions = []

    lines = text_output.split('\n')
    for line in lines:
        # lizard æ ¼å¼: N lines N tokens N CC N params N loc file:class:function
        parts = line.split()
        if len(parts) >= 8:
            try:
                tokens = int(parts[1])
                if tokens == 0:  # è·³è¿‡æ‘˜è¦è¡Œ
                    continue

                cc = int(parts[2])
                file_path = parts[-1]
                
                # è§£æ file:class:function
                if ':' in file_path:
                    file_parts = file_path.split(':')
                    if len(file_parts) >= 3:
                        file_name = ':'.join(file_parts[:-2])
                        class_name = file_parts[-2]
                        function_name = file_parts[-1]
                    else:
                        file_name = file_parts[0]
                        class_name = ''
                        function_name = file_parts[-1] if len(file_parts) > 1 else ''
                else:
                    file_name = file_path
                    class_name = ''
                    function_name = ''

                key = f"{file_name}:{class_name}.{function_name}" if class_name else f"{file_name}:{function_name}"

                files[key] = {
                    'type': 'function',
                    'complexity': cc,
                    'file': file_name,
                    'class': class_name,
                    'method': function_name,
                    'tokens': tokens,
                }
            except (ValueError, IndexError):
                continue

    return organize_complexity_data(files, 'lizard')


def organize_complexity_data(files, tool):
    """ç»„ç»‡å¤æ‚åº¦æ•°æ®"""
    if not files:
        return None

    # æŒ‰å¤æ‚åº¦æ’åº
    sorted_files = sorted(files.items(), key=lambda x: x[1]['complexity'], reverse=True)

    # è®¡ç®—ç»Ÿè®¡
    complexities = [f['complexity'] for f in files.values()]
    total_functions = len(files)
    avg_complexity = sum(complexities) / total_functions if total_functions > 0 else 0
    max_complexity = max(complexities) if complexities else 0

    # æŒ‰é£é™©ç­‰çº§åˆ†ç±»
    risk_levels = {
        'low': [],      # CC < 15
        'medium': [],   # CC 15-25
        'high': [],     # CC 25-50
        'critical': [], # CC > 50
    }

    for key, data in files.items():
        cc = data['complexity']
        if cc < 15:
            risk_levels['low'].append((key, cc))
        elif cc < 25:
            risk_levels['medium'].append((key, cc))
        elif cc < 50:
            risk_levels['high'].append((key, cc))
        else:
            risk_levels['critical'].append((key, cc))

    # æŒ‰æ–‡ä»¶åˆ†ç»„
    file_complexity = defaultdict(list)
    for key, data in files.items():
        file_path = data.get('file', key.split(':')[0])
        file_complexity[file_path].append(data['complexity'])

    file_avg_complexity = {}
    for file_path, ccs in file_complexity.items():
        file_avg_complexity[file_path] = sum(ccs) / len(ccs)

    return {
        'tool': tool,
        'total_functions': total_functions,
        'avg_complexity': avg_complexity,
        'max_complexity': max_complexity,
        'sorted_files': sorted_files,
        'risk_levels': risk_levels,
        'file_complexity': dict(sorted(
            file_avg_complexity.items(),
            key=lambda x: x[1],
            reverse=True
        )),
    }


def get_complexity_level(cc):
    """è·å–å¤æ‚åº¦ç­‰çº§"""
    if cc < 15:
        return 'Low', 'ğŸŸ¢'
    elif cc < 25:
        return 'Medium', 'ğŸŸ¡'
    elif cc < 50:
        return 'High', 'ğŸŸ '
    else:
        return 'Critical', 'ğŸ”´'


def generate_complexity_bar(cc, width=20):
    """ç”Ÿæˆå¤æ‚åº¦å¯è§†åŒ–æ¡"""
    if cc >= 50:
        filled = width
    else:
        filled = min(int(cc / 50 * width), width)
    bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
    return f"[{bar}] {cc}"


def generate_report(data):
    """ç”Ÿæˆå¤æ‚åº¦æŠ¥å‘Š"""
    if not data:
        return generate_no_analysis_report()

    lines = []
    lines.append("=" * 80)
    lines.append("ğŸ“Š ä»£ç å¤æ‚åº¦åˆ†ææŠ¥å‘Š (Code Complexity Analysis)")
    lines.append(f"â° åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"ğŸ”§ åˆ†æå·¥å…·: {data['tool'].upper()}")
    lines.append("=" * 80)
    lines.append("")

    # æ‘˜è¦ç»Ÿè®¡
    lines.append("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
    lines.append("-" * 80)
    lines.append(f"åˆ†æå‡½æ•°æ€»æ•°: {data['total_functions']:,}")
    lines.append(f"å¹³å‡å¤æ‚åº¦: {data['avg_complexity']:.2f}")
    lines.append(f"æœ€é«˜å¤æ‚åº¦: {data['max_complexity']}")
    lines.append("")

    # é£é™©åˆ†å¸ƒ
    lines.append("ğŸ¯ é£é™©ç­‰çº§åˆ†å¸ƒ")
    lines.append("-" * 80)
    risk_map = {
        'low': ('ğŸŸ¢', 'ä½é£é™© (CC < 15)'),
        'medium': ('ğŸŸ¡', 'ä¸­é£é™© (CC 15-25)'),
        'high': ('ğŸŸ ', 'é«˜é£é™© (CC 25-50)'),
        'critical': ('ğŸ”´', 'ç´§æ€¥é£é™© (CC > 50)'),
    }

    for level, (emoji, desc) in risk_map.items():
        count = len(data['risk_levels'][level])
        if count > 0:
            lines.append(f"{emoji} {desc}: {count} ä¸ªå‡½æ•°")

    lines.append("")

    # ç´§æ€¥é£é™©å‡½æ•°
    if data['risk_levels']['critical']:
        lines.append("ğŸ”´ ç´§æ€¥é£é™©å‡½æ•° (CC > 50)")
        lines.append("-" * 80)
        for key, cc in data['risk_levels']['critical'][:10]:
            lines.append(f"  {generate_complexity_bar(cc, 20)} {key}")
        if len(data['risk_levels']['critical']) > 10:
            lines.append(f"  ... è¿˜æœ‰ {len(data['risk_levels']['critical']) - 10} ä¸ª")
        lines.append("")

    # é«˜é£é™©å‡½æ•°
    if data['risk_levels']['high']:
        lines.append("ğŸŸ  é«˜é£é™©å‡½æ•° (CC 25-50)")
        lines.append("-" * 80)
        for key, cc in data['risk_levels']['high'][:15]:
            lines.append(f"  {generate_complexity_bar(cc, 15)} {key}")
        if len(data['risk_levels']['high']) > 15:
            lines.append(f"  ... è¿˜æœ‰ {len(data['risk_levels']['high']) - 15} ä¸ª")
        lines.append("")

    # Top 20 æœ€å¤æ‚å‡½æ•°
    lines.append("ğŸ” Top 20 æœ€å¤æ‚å‡½æ•°")
    lines.append("-" * 80)
    lines.append(f"{'å¤æ‚åº¦':<30} {'å‡½æ•°'}")
    lines.append("-" * 80)

    for i, (key, func_data) in enumerate(data['sorted_files'][:20], 1):
        cc = func_data['complexity']
        level, emoji = get_complexity_level(cc)
        bar = generate_complexity_bar(cc, 20)

        # æ„å»ºæ˜¾ç¤ºåç§°
        if func_data.get('class'):
            display_name = f"{func_data['file']}:{func_data['class']}.{func_data['method']}"
        elif func_data.get('method'):
            display_name = f"{func_data['file']}:{func_data['method']}"
        else:
            display_name = key

        lines.append(f"{i:2}. {bar} {emoji}")
        lines.append(f"     {display_name}")

    lines.append("")

    # æ–‡ä»¶å¤æ‚åº¦æ’å
    lines.append("ğŸ“ æ–‡ä»¶å¤æ‚åº¦æ’å")
    lines.append("-" * 80)
    lines.append(f"{'å¹³å‡å¤æ‚åº¦':<30} {'æ–‡ä»¶'}")
    lines.append("-" * 80)

    for i, (file_path, avg_cc) in enumerate(list(data['file_complexity'].items())[:20], 1):
        level, emoji = get_complexity_level(avg_cc)
        bar = generate_complexity_bar(avg_cc, 15)
        lines.append(f"{i:2}. {bar} {emoji} {file_path}")

    lines.append("")

    # æ”¹è¿›å»ºè®®
    lines.append("ğŸ’¡ æ”¹è¿›å»ºè®®")
    lines.append("-" * 80)

    critical_count = len(data['risk_levels']['critical'])
    high_count = len(data['risk_levels']['high'])
    medium_count = len(data['risk_levels']['medium'])

    if critical_count > 0:
        lines.append(f"ğŸ”´ ç´§æ€¥: {critical_count} ä¸ªå‡½æ•°å¤æ‚åº¦è¶…è¿‡ 50ï¼Œå¿…é¡»é‡æ„")

    if high_count > 0:
        lines.append(f"ğŸŸ  é‡è¦: {high_count} ä¸ªå‡½æ•°å¤æ‚åº¦åœ¨ 25-50 ä¹‹é—´")

    if medium_count > 0:
        lines.append(f"ğŸŸ¡ å»ºè®®: {medium_count} ä¸ªå‡½æ•°å¤æ‚åº¦åœ¨ 15-25 ä¹‹é—´")

    if data['avg_complexity'] > 20:
        lines.append("")
        lines.append("âš ï¸  è­¦å‘Š: é¡¹ç›®å¹³å‡å¤æ‚åº¦åé«˜ï¼Œå»ºè®®æ•´ä½“é‡æ„")

    # é‡æ„ä¼˜å…ˆçº§
    lines.append("")
    lines.append("ğŸ¯ é‡æ„ä¼˜å…ˆçº§:")

    priority_list = []
    priority_list.extend(data['risk_levels']['critical'])
    priority_list.extend(data['risk_levels']['high'])

    if priority_list:
        for i, (key, cc) in enumerate(priority_list[:5], 1):
            lines.append(f"  {i}. {key}")
            lines.append(f"     å½“å‰å¤æ‚åº¦: {cc}, ç›®æ ‡: < 15")
    else:
        lines.append("  âœ… å½“å‰ä»£ç å¤æ‚åº¦åœ¨å¯æ¥å—èŒƒå›´å†…")

    lines.append("")
    lines.append("=" * 80)
    lines.append("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    lines.append("=" * 80)

    return '\n'.join(lines)


def generate_no_analysis_report():
    """ç”Ÿæˆæ— åˆ†ææ•°æ®æ—¶çš„æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append("ğŸ“Š ä»£ç å¤æ‚åº¦åˆ†ææŠ¥å‘Š (Code Complexity Analysis)")
    lines.append(f"â° åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("=" * 80)
    lines.append("")
    lines.append("âš ï¸  æœªæ‰¾åˆ°å¯åˆ†æçš„ä»£ç æˆ–åˆ†æå·¥å…·æœªå®‰è£…")
    lines.append("")
    lines.append("è¯·å®‰è£…åˆ†æå·¥å…·ï¼š")
    lines.append("")
    lines.append("Python é¡¹ç›® (æ¨è radon):")
    lines.append("  pip install radon")
    lines.append("")
    lines.append("å¤šè¯­è¨€é¡¹ç›® (æ¨è lizard):")
    lines.append("  pip install lizard")
    lines.append("")
    lines.append("ç„¶åé‡æ–°è¿è¡Œåˆ†æ")
    lines.append("")
    lines.append("=" * 80)

    return '\n'.join(lines)


def save_report(report, output_file='complexity_map_report.txt'):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    print("ğŸ” ä»£ç å¤æ‚åº¦åˆ†æå·¥å…·")
    print()

    # å°è¯•ä½¿ç”¨ä¸åŒçš„å·¥å…·è¿›è¡Œåˆ†æ
    data = None

    # ä¼˜å…ˆä½¿ç”¨ radon (Python)
    if os.path.exists('setup.py') or os.path.exists('pyproject.toml') or os.path.exists('pytest.ini'):
        print("ğŸ æ£€æµ‹åˆ° Python é¡¹ç›®")
        data = analyze_with_radon()

    # å¦‚æœ radon å¤±è´¥ï¼Œå°è¯• lizard
    if not data:
        data = analyze_with_lizard()

    print()
    print("ğŸ“ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")

    report = generate_report(data)

    print()
    print(report)

    save_report(report)


if __name__ == '__main__':
    main()
