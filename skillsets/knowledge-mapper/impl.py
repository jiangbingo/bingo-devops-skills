#!/usr/bin/env python3
"""
Knowledge Mapper - é¡¹ç›®çŸ¥è¯†å›¾è°±æ˜ å°„
"""

import subprocess
import re
from collections import defaultdict
from pathlib import Path

# éœ€è¦æ’é™¤çš„æ–‡ä»¶æ¨¡å¼
EXCLUDE_PATTERNS = [
    r'node_modules/',
    r'vendor/',
    r'\.git/',
    r'dist/',
    r'build/',
    r'\.venv/',
    r'venv/',
    r'__pycache__/',
    r'\.pyc$',
    r'\.min\.js$',
    r'\.min\.css$',
    r'package-lock\.json',
    r'yarn\.lock',
    r'Pods/',
    r'\.xcodeproj/',
    r'\.xcworkspace/',
    r'DerivedData/',
    r'\.md$',  # æ’é™¤æ–‡æ¡£æ–‡ä»¶
    r'\.txt$',
    r'\.json$',
    r'\.yaml$',
    r'\.yml$',
]

def should_exclude(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
    for pattern in EXCLUDE_PATTERNS:
        if re.search(pattern, file_path):
            return True
    return False

def get_git_root():
    """è·å– Git ä»“åº“æ ¹ç›®å½•"""
    result = subprocess.run(
        ['git', 'rev-parse', '--show-toplevel'],
        capture_output=True,
        text=True
    )
    if result.returncode != 0:
        return None
    return result.stdout.strip()

def get_author_file_mapping():
    """è·å–ä½œè€…ä¸æ–‡ä»¶çš„æ˜ å°„å…³ç³»"""
    result = subprocess.run([
        'git', 'log',
        '--pretty=format:%an',
        '--name-only',
        '-m',
    ], capture_output=True, text=True)

    if result.returncode != 0:
        return {}, {}

    lines = result.stdout.strip().split('\n')
    author_file_data = defaultdict(lambda: defaultdict(int))
    file_author_data = defaultdict(lambda: defaultdict(int))

    current_author = None
    for line in lines:
        line = line.strip()
        if not line:
            continue

        if not should_exclude(line) and '/' in line:
            # è¿™æ˜¯æ–‡ä»¶è·¯å¾„
            if current_author:
                author_file_data[current_author][line] += 1
                file_author_data[line][current_author] += 1
        else:
            # è¿™æ˜¯ä½œè€…å
            current_author = line

    return dict(author_file_data), dict(file_author_data)

def analyze_code_ownership(file_author_data):
    """åˆ†æä»£ç æ‰€æœ‰æƒ"""
    file_ownership = {}

    for file_path, authors in file_author_data.items():
        if should_exclude(file_path):
            continue

        total_commits = sum(authors.values())
        sorted_authors = sorted(authors.items(), key=lambda x: x[1], reverse=True)

        primary_owner = sorted_authors[0][0] if sorted_authors else "Unknown"
        contributor_count = len(authors)

        # è®¡ç®—æ‰€æœ‰æƒé›†ä¸­åº¦ (ä¸»è´¡çŒ®è€…å æ¯”)
        primary_ratio = sorted_authors[0][1] / total_commits if total_commits > 0 else 0

        file_ownership[file_path] = {
            'primary_owner': primary_owner,
            'contributors': list(authors.keys()),
            'contributor_count': contributor_count,
            'total_commits': total_commits,
            'ownership_concentration': primary_ratio,
        }

    return file_ownership

def calculate_knowledge_risk(file_ownership):
    """è®¡ç®—çŸ¥è¯†é£é™©ç­‰çº§"""
    risk_analysis = {}

    for file_path, data in file_ownership.items():
        contributor_count = data['contributor_count']

        if contributor_count == 1:
            risk_level = "Critical"
            risk_emoji = "ğŸ”´"
        elif contributor_count == 2:
            risk_level = "High"
            risk_emoji = "ğŸŸ "
        elif contributor_count <= 5:
            risk_level = "Medium"
            risk_emoji = "ğŸŸ¡"
        else:
            risk_level = "Low"
            risk_emoji = "ğŸŸ¢"

        risk_analysis[file_path] = {
            'level': risk_level,
            'emoji': risk_emoji,
            'contributor_count': contributor_count,
        }

    return risk_analysis

def find_file_relationships(author_file_data):
    """æ‰¾å‡ºæ–‡ä»¶é—´çš„å…³ç³»ï¼ˆåŸºäºå…±åŒä¿®æ”¹è€…ï¼‰"""
    file_cooccurrence = defaultdict(lambda: defaultdict(int))

    for author, files in author_file_data.items():
        file_list = list(files.keys())
        # è®¡ç®—æ–‡ä»¶å…±ç°
        for i, file1 in enumerate(file_list):
            for file2 in file_list[i+1:]:
                if not should_exclude(file1) and not should_exclude(file2):
                    file_cooccurrence[file1][file2] += 1
                    file_cooccurrence[file2][file1] += 1

    # æ‰¾å‡ºå¼ºå…³è”ï¼ˆå…±åŒä¿®æ”¹æ¬¡æ•° >= 2ï¼‰
    strong_relationships = []
    for file1, related in file_cooccurrence.items():
        for file2, count in related.items():
            if count >= 2:
                strong_relationships.append((file1, file2, count))

    strong_relationships.sort(key=lambda x: x[2], reverse=True)
    return strong_relationships

def generate_dot_graph(file_ownership, strong_relationships, output_file):
    """ç”Ÿæˆ Graphviz DOT æ ¼å¼çš„çŸ¥è¯†å›¾è°±"""
    dot_content = []
    dot_content.append('digraph KnowledgeGraph {')
    dot_content.append('  rankdir=LR;')
    dot_content.append('  node [shape=box, style=rounded];')
    dot_content.append('')

    # æŒ‰æ¨¡å—åˆ†ç»„æ–‡ä»¶
    modules = defaultdict(list)
    for file_path in file_ownership.keys():
        if '/' in file_path:
            module = file_path.split('/')[0]
        else:
            module = 'root'
        modules[module].append(file_path)

    # åˆ›å»ºå­å›¾
    for module, files in modules.items():
        if len(files) > 1:
            dot_content.append(f'  subgraph cluster_{module} {{')
            dot_content.append(f'    label="{module}";')
            dot_content.append(f'    style=filled;')
            dot_content.append(f'    color=lightgrey;')
            for file_path in files[:10]:  # é™åˆ¶æ¯ä¸ªæ¨¡å—æœ€å¤š10ä¸ªæ–‡ä»¶
                safe_name = file_path.replace('/', '_').replace('.', '_').replace('-', '_')
                risk = file_ownership[file_path]['contributor_count']
                color = "red" if risk <= 2 else "yellow" if risk <= 5 else "green"
                dot_content.append(f'    "{safe_name}" [label="{file_path}", fillcolor={color}, style="rounded,filled"];')
            dot_content.append('  }')
            dot_content.append('')

    # æ·»åŠ è¾¹ï¼ˆæ–‡ä»¶å…³ç³»ï¼‰
    for file1, file2, count in strong_relationships[:50]:  # é™åˆ¶è¾¹æ•°é‡
        safe_name1 = file1.replace('/', '_').replace('.', '_').replace('-', '_')
        safe_name2 = file2.replace('/', '_').replace('.', '_').replace('-', '_')
        dot_content.append(f'  "{safe_name1}" -> "{safe_name2}" [label="{count}", penwidth={min(count, 3)}];')

    dot_content.append('}')
    dot_content.append('')

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(dot_content))

def generate_report(author_file_data, file_author_data, file_ownership, risk_analysis, strong_relationships):
    """ç”ŸæˆçŸ¥è¯†å›¾è°±åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 140)
    report.append("é¡¹ç›®çŸ¥è¯†å›¾è°±åˆ†ææŠ¥å‘Š (Knowledge Map Analysis)")
    report.append("=" * 140)
    report.append("")

    # åŸºæœ¬ç»Ÿè®¡
    total_authors = len(author_file_data)
    total_files = len(file_ownership)
    total_relationships = len(strong_relationships)

    report.append("ğŸ“Š åŸºæœ¬ç»Ÿè®¡")
    report.append("=" * 140)
    report.append(f"æ€»è´¡çŒ®è€…æ•°: {total_authors}")
    report.append(f"åˆ†ææ–‡ä»¶æ•°: {total_files}")
    report.append(f"æ–‡ä»¶å…³è”æ•°: {total_relationships}")
    report.append("")

    # è´¡çŒ®è€…æ’è¡Œ
    report.append("=" * 140)
    report.append("ğŸ‘¥ è´¡çŒ®è€…æ’è¡Œ (æŒ‰æ–‡ä»¶ä¿®æ”¹æ•°)")
    report.append("=" * 140)

    author_file_counts = [(author, sum(files.values())) for author, files in author_file_data.items()]
    author_file_counts.sort(key=lambda x: x[1], reverse=True)

    for i, (author, count) in enumerate(author_file_counts[:20], 1):
        percentage = (count / sum(c for _, c in author_file_counts) * 100) if author_file_counts else 0
        report.append(f"  {i:2}. {author:<30} ä¿®æ”¹æ–‡ä»¶: {count:<4} ({percentage:.1f}%)")

    report.append("")

    # çŸ¥è¯†é£é™©åˆ†æ
    report.append("=" * 140)
    report.append("âš ï¸  çŸ¥è¯†é£é™©åˆ†æ (Bus Factor)")
    report.append("=" * 140)

    risk_counts = defaultdict(int)
    for risk in risk_analysis.values():
        risk_counts[risk['level']] += 1

    report.append(f"ğŸ”´ Critical é£é™© (1äºº): {risk_counts['Critical']} ä¸ªæ–‡ä»¶")
    report.append(f"ğŸŸ  High é£é™© (2äºº):    {risk_counts['High']} ä¸ªæ–‡ä»¶")
    report.append(f"ğŸŸ¡ Medium é£é™© (3-5äºº): {risk_counts['Medium']} ä¸ªæ–‡ä»¶")
    report.append(f"ğŸŸ¢ Low é£é™© (6+äºº):   {risk_counts['Low']} ä¸ªæ–‡ä»¶")
    report.append("")

    # åˆ—å‡ºé«˜é£é™©æ–‡ä»¶
    high_risk_files = [(fp, d) for fp, d in file_ownership.items() if risk_analysis[fp]['level'] in ['Critical', 'High']]
    high_risk_files.sort(key=lambda x: x[1]['contributor_count'])

    if high_risk_files:
        report.append("é«˜é£é™©æ–‡ä»¶åˆ—è¡¨:")
        report.append("")
        for file_path, data in high_risk_files[:30]:
            risk = risk_analysis[file_path]
            report.append(f"  {risk['emoji']} {file_path}")
            report.append(f"     ä¸»è¦è´¡çŒ®è€…: {data['primary_owner']}")
            report.append(f"     è´¡çŒ®è€…æ•°: {data['contributor_count']} | æ€»æäº¤: {data['total_commits']}")
            report.append("")

    # ä»£ç æ‰€æœ‰æƒæŠ¥å‘Š
    report.append("=" * 140)
    report.append("ğŸ“ ä»£ç æ‰€æœ‰æƒæŠ¥å‘Š (Top 30 æ–‡ä»¶)")
    report.append("=" * 140)
    report.append(f"{'æ–‡ä»¶è·¯å¾„':<50} {'ä¸»è¦è´¡çŒ®è€…':<20} {'è´¡çŒ®è€…æ•°':<8} {'é›†ä¸­åº¦':<10} {'é£é™©ç­‰çº§'}")
    report.append("-" * 140)

    sorted_files = sorted(file_ownership.items(), key=lambda x: x[1]['total_commits'], reverse=True)

    for file_path, data in sorted_files[:30]:
        primary = data['primary_owner']
        contributors = data['contributor_count']
        concentration = f"{data['ownership_concentration']*100:.0f}%"
        risk = risk_analysis[file_path]['emoji'] + " " + risk_analysis[file_path]['level']

        display_path = file_path if len(file_path) <= 48 else '...' + file_path[-45:]
        report.append(f"{display_path:<50} {primary:<20} {contributors:<8} {concentration:<10} {risk}")

    report.append("")

    # ä¸“å®¶é¢†åŸŸè¯†åˆ«
    report.append("=" * 140)
    report.append("ğŸ¯ ä¸“å®¶é¢†åŸŸè¯†åˆ«")
    report.append("=" * 140)

    # æ‰¾å‡ºæ¯ä¸ªä½œè€…çš„ä¸“é•¿é¢†åŸŸ
    author_expertise = defaultdict(lambda: defaultdict(int))
    for author, files in author_file_data.items():
        for file_path, count in files.items():
            if '/' in file_path:
                module = '/'.join(file_path.split('/')[:2])  # å–å‰ä¸¤çº§ç›®å½•ä½œä¸ºæ¨¡å—
            else:
                module = file_path
            author_expertise[author][module] += count

    for author in author_file_counts[:10]:
        author_name = author[0]
        modules = sorted(author_expertise[author_name].items(), key=lambda x: x[1], reverse=True)[:5]
        if modules:
            report.append(f"\n  {author_name}:")
            for module, count in modules:
                report.append(f"    - {module} ({count} æ¬¡ä¿®æ”¹)")

    report.append("")

    # æ–‡ä»¶å…³è”åˆ†æ
    if strong_relationships:
        report.append("=" * 140)
        report.append("ğŸ”— æ–‡ä»¶å…³è”åˆ†æ (å¼ºå…³è”æ–‡ä»¶å¯¹)")
        report.append("=" * 140)
        report.append("ä»¥ä¸‹æ–‡ä»¶ç»å¸¸è¢«ä¸€èµ·ä¿®æ”¹ï¼Œå¯èƒ½å­˜åœ¨é€»è¾‘ä¾èµ–å…³ç³»:")
        report.append("")

        for file1, file2, count in strong_relationships[:20]:
            report.append(f"  {count:3} æ¬¡: {file1}")
            report.append(f"         {file2}")
            report.append("")

    # å»ºè®®
    report.append("=" * 140)
    report.append("ğŸ’¡ å»ºè®®")
    report.append("=" * 140)

    critical_count = risk_counts['Critical']
    high_count = risk_counts['High']

    if critical_count > 0:
        report.append("")
        report.append(f"ğŸš¨ å‘ç° {critical_count} ä¸ª Critical é£é™©æ–‡ä»¶ï¼ˆå•äººè´Ÿè´£ï¼‰:")
        report.append("  - ç«‹å³ä¸ºè¿™äº›æ–‡ä»¶æŒ‡å®šå¤‡ä»½è´£ä»»äºº")
        report.append("  - é€šè¿‡ä»£ç å®¡æŸ¥è®©å…¶ä»–å›¢é˜Ÿæˆå‘˜ç†Ÿæ‚‰ä»£ç ")
        report.append("  - è€ƒè™‘é‡å†™æˆ–ç®€åŒ–è¿™äº›æ–‡ä»¶")

    if high_count > 0:
        report.append("")
        report.append(f"âš ï¸  å‘ç° {high_count} ä¸ª High é£é™©æ–‡ä»¶ï¼ˆåŒäººè´Ÿè´£ï¼‰:")
        report.append("  - æ‰©å±•è¿™äº›æ–‡ä»¶çš„ç†Ÿæ‚‰äººæ•°")
        report.append("  - åœ¨å›¢é˜Ÿä¸­è¿›è¡ŒçŸ¥è¯†åˆ†äº«")

    report.append("")
    report.append("é€šç”¨å»ºè®®:")
    report.append("  - å®šæœŸè¿è¡Œæ­¤åˆ†æç›‘æ§çŸ¥è¯†åˆ†å¸ƒ")
    report.append("  - å¯¹é«˜é£é™©æ–‡ä»¶å®æ–½ç»“å¯¹ç¼–ç¨‹")
    report.append("  - å»ºç«‹ä»£ç å®¡æŸ¥è½®æ¢åˆ¶åº¦")
    report.append("  - ç»´æŠ¤ä»£ç æ–‡æ¡£ä»¥é™ä½çŸ¥è¯†å­¤å²›é£é™©")

    return '\n'.join(report)

def save_report(report, output_file):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

def main():
    print("ğŸ” æ­£åœ¨åˆ†æé¡¹ç›®çŸ¥è¯†å›¾è°±...")

    git_root = get_git_root()
    if not git_root:
        print("âŒ é”™è¯¯: å½“å‰ç›®å½•ä¸æ˜¯ Git ä»“åº“")
        return

    print(f"âœ… Git ä»“åº“æ ¹ç›®å½•: {git_root}")

    print("ğŸ“Š æ­£åœ¨è·å–ä½œè€…-æ–‡ä»¶æ˜ å°„...")
    author_file_data, file_author_data = get_author_file_mapping()
    print(f"âœ… è·å–åˆ° {len(author_file_data)} ä¸ªè´¡çŒ®è€…")

    print("ğŸ“Š æ­£åœ¨åˆ†æä»£ç æ‰€æœ‰æƒ...")
    file_ownership = analyze_code_ownership(file_author_data)
    print(f"âœ… åˆ†æäº† {len(file_ownership)} ä¸ªæ–‡ä»¶")

    print("ğŸ“Š æ­£åœ¨è®¡ç®—çŸ¥è¯†é£é™©...")
    risk_analysis = calculate_knowledge_risk(file_ownership)

    print("ğŸ”— æ­£åœ¨åˆ†ææ–‡ä»¶å…³è”...")
    strong_relationships = find_file_relationships(author_file_data)
    print(f"âœ… å‘ç° {len(strong_relationships)} ä¸ªæ–‡ä»¶å…³è”")

    print("ğŸ“ æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report = generate_report(author_file_data, file_author_data, file_ownership, risk_analysis, strong_relationships)

    output_file = 'knowledge_map_report.txt'
    save_report(report, output_file)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    # ç”Ÿæˆ DOT å›¾
    dot_file = 'knowledge_graph.dot'
    print(f"ğŸ“Š æ­£åœ¨ç”ŸæˆçŸ¥è¯†å›¾è°±...")
    generate_dot_graph(file_ownership, strong_relationships, dot_file)
    print(f"âœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {dot_file}")
    print(f"   ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯è§†åŒ–: dot -Tpng {dot_file} -o knowledge_graph.png")

    print("\n" + "=" * 60)
    print("ğŸ“‹ åˆ†ææ‘˜è¦")
    print("=" * 60)
    print(f"  è´¡çŒ®è€…æ•°: {len(author_file_data)}")
    print(f"  åˆ†ææ–‡ä»¶: {len(file_ownership)}")

    risk_counts = defaultdict(int)
    for risk in risk_analysis.values():
        risk_counts[risk['level']] += 1
    print(f"  ğŸ”´ Critical: {risk_counts['Critical']}")
    print(f"  ğŸŸ  High:     {risk_counts['High']}")
    print(f"  ğŸŸ¡ Medium:   {risk_counts['Medium']}")
    print(f"  ğŸŸ¢ Low:      {risk_counts['Low']}")
    print(f"  æŠ¥å‘Šæ–‡ä»¶: {output_file}")

if __name__ == '__main__':
    main()
